#환경 설정
gpu_id: 'cuda:0' #str. 사용가능한 gpuid 이거 어차피 잘 안됨. 그냥 직접 to(device)하는게 현명
worker: 8 #int. numworker
seed: 42 #int, 시드
state: 'INFO' #str, 로그 출력 수준. ['INFO', 'DEBUG', 'WARNING', 'ERROR', 'CRITICAL']
reproducibility: True #bool. 잘 몰?루 재생산 가능하게 하는 방법과, 더 빠르지만 안 되게 하는 방법
# data_path: 'movie'
checkpoint_dir: 'saved/'
show_progress : True #tqdm으로 진행사항 볼 것인지
log_wandb: False
shuffle: True # 에폭 돌기 전에 학습데이터 섞을지

#데이터 설정
field_separator: "\t" # tsv 형태? csv 형태?
seq_separator: " " # 시퀀스 구분자

    #기본
USER_ID_FIELD: 'user' #유저
ITEM_ID_FIELD: 'item' #아이템
RATING_FIELD: 'rating' #평점. 어느 모델이든 얘네들 있든 없든 상관은 없다.
TIME_FIELD: 'timestamp' # for sequential model
# seq_len: None

    #for point-wise 라벨
# LABEL_FIELD: label # 정답라벨

load_col: # 사용할 피쳐.
    inter: ['user', 'item', 'rating', 'timestamp'] # inter에 있는 피쳐니까 이렇게 표시
    user: ['user']
    item: ['item', 'year', 'title', 'genre']
val_interval:
    rating: "[0,1]"
user_inter_num_interval: "[0,inf)"
item_inter_num_interval: "[0,inf)"

# 학습 설정
epochs: 300
train_batch_size: 2048
learner: 'adam'
learning_rate : 0.001
train_neg_sample_args : # 오답 샘플링
    distribution : 'uniform' # ['uniform', 'popularity'] 오답 분포 정하기
    sample_num : 1 # 한 정답 대비 오답 갯수
    dynamic : False # 동적으로? 실시간으로 오답을 하는지?
    candidate_num : 0 #

# eval_step : 1 # 학습 중 언제부터 검증셋을 쓸지
stopping_step : 3 # 얼리스타핑 patience
# # clip_grad_norm : None
# loss_decimal_place : 4 # The decimal place of training loss
# weight_decay : 0.0 # weight decay (L2 penalty), used for optimizer.

# eval 설정
eval_args :
    group_by : user # 무얼 기준으로 나눌지. none으로 두면 group하지 않는다.
    order : 'RO' # inter에서 데이터 정렬할지. RO는 랜덤하게, TO는 시간 순으로
    split :
        RS : [0.999, 0.001, 0] # train,valid,test 나누는 비율. LS나 RS 하나만 써라.
        #LS : 'test_only' #LS로 하면 ['valid_and_test', 'valid_only', 'test_only']
    mode : 'full' # ['full','unixxx','popxxx','labeled'] 모델로 평가할 데이터 범위를 정한다.
                    # labeled 빼면 전부 암묵적 피드백에 대한 것. xxx에 숫자를 넣는다. 오답을 몇개 둘지!
repeatable : True #본거에 대해서 추천을 해줄지. 시퀀스에 대해서는 작동하지 않는다.
metrics : ['Recall', 'MRR', 'NDCG', 'Hit']
        # 랭킹 지표 Recall, MRR, NDCG, Hit, MAP, Precision, GAUC, ItemCoverage, AveragePopularity, GiniIndex, ShannonEntropy, TailPercentage
        # 평점 지표 AUC, MAE, RMSE, LogLoss
topk : 10 #평가 시에 몇 개 추천할지
valid_metric : 'Recall@10' # 얼리스타핑에 사용할 지표. 우리의 정답지에 애초에 순위가 안 나와있다. 순위를 고려한 지표를 걸수는 없다.
eval_batch_size: 4096 #평가 시의 배치 사이즈

test_args:


# embedding_size: 128  0.1086 -> 0.1204 -> ...
# reg_weight : 750